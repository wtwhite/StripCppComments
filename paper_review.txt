The paper describes the challenges of retrofitting provenance mechanisms to existing software in such a way that the needs of all stakeholders (including anyone affected by decisions resulting from running the software, direct end users, software developers and regulators) are met or at least balanced against each other, while minimising both the one-off and ongoing costs of doing so. Important aspects of provenance include the ability to see what algorithms and input data were used to produce the output, as well as the ability to readily understand or interpret them. One key challenges is the fact that provenance data can often dwarf the size of the actual "real" data output by a ststem.

A running example of an insurance premium calculation is used for illustration.

Existing approaches to provenance include static and dynamic program analysis techniques, like that behind GitHub's Dependabot. These run into hard theoretical limits (non-decidability of the Halting Problem) but in practice can have high impact on the kinds of programs encountered in practice. Another is the use of provenance-generating monads built into a functional language to generate provenance information automatically.

Questions for the authors:

1. Regarding "Regulations such as GDPR and HIPPA raise the question of how organizations can show compliance to both regulators": It is hard to see how it could ever be possible to convincingly demonstrate that one *has not* done something, for instance that one has not shared health records per HIPAA. To the best of my knowledge, the only way to achieve any genuine confidence there is to actually attempt to persuade the party in question to share the records, and observe that they don't (this could be thought of as "social red-teaming"). But this leads to many ethical and practical issues there. Other kinds of "auditing", like checking that an organisation uses a particular protocol or service, are inherently weaker since they rely on trust. Has this "adversarial" approach been considered?
2. Regarding privacy concerns, can the idea of k-anonymity be incorporated somehow?

Ideas:

The approaches to provenance described in the paper are likely very effective in practice, but from a theoretical point of view are largely "weak" in the sense that they are unable to defend some parties from another party "just always lying". A different approach is to look for (cryptographically) strong conceptions of provenance -- that is, formalisations that lead to mechanisms with mathematical guarantees against even the possibility of one party deceiving another.

This seems to imply much tigher tracking, which decreases anonymity, and could lead to some players (e.g., PKI certificate authorities) acquiring disproportionate power. This is undesirable, but it seems inevitable that anonymity is in tension with such strong conceptions of provenance. With that in mind, some ideas towards improving provenance via cryptographically strong approaches to non-repudiation follow:

- To track who wrote code, public-key cryptography can be used with a PKI or web of trust and integrated into source control systems, e.g., signed git commits.
- To track where code was built, compilers could append MACs to executables based on a key from a hidden hardware ID (e.g., from a TPM). This involves trusting the hardware vendor.
- "Authenticating compilation" could be used to track who built the code: After a developer "logs in" to a remote compilation service with their per-developer private key, it can append "built-by-user" and "built-by-compiler" MACs to output executables. "built-by-user" is like signed JARs; "built-by-compiler" requires the compiler executable to have its own private key, which is what requires the compilation service to be remote.
- As an alternative to MACs, compilers could "watermark" compiled code (e.g., through the choice of semantically equivalent orderings of instructions or data objects in memory). Watermarks increase confidence as code size increases, and allow any (small) amount of information to be easily included, but still require trusting the compiler author (and all authors in its "compilation history", per "Reflections on Trusting Trust" (Thompson 1984)).
- To track where code was run, and which code was run to produce a given piece of output data, it might be possible to design a MAC-like algorithm that a CPU computes while running a given program p. Specifically, it may be possible to design functions f(), g() and s() in such a way that, while running a program p, a CPU can compute y = f(machinePrivateKey, p, s(executionTrace(p))) incrementally as p runs and output y on program termination, and another party can later compute z = g(machinePublicKey, y) and learn from z with some certainty whether the machine in question produced that hash y. Given that we want to avoid having to store and transmit the execution trace data itself, and with execution traces being highly dependent on (unknown-to-observers) input data (and even highly nondeterministic for known data), this would seem to require that s() throw away nearly all information. E.g., s() could simply return the length L of the execution trace multiplied by some large constant M, f() could encrypt L*hash(initialMemoryImage(p)) using machinePrivateKey to produce y, and g() could attempt to decode y using the given public key to give z. If an observer computes z = g(machinePublicKey, y) and finds both z % M == 0 and z % hash(initialMemoryImage(p)) == 0, then they can feel somewhat confident that the program output was in fact generated by running p on that machine (non-repudiation). This approach has many practical complications (What constitutes the "program image"? How should computations be combined for processing that spans processes?).
- Regulations could require that all data produced by an organisation be accompanied by the y value of the program run used to produce it and the hash of the program itself. This makes the run auditable (but entails a lot of work). Separately, to enable transparency, the source code of all programs whose hashes appear in the former could be required to be made available (for a period of time, etc.).
- There might already exist cryptographic primitives allowing verification of weaker claims such as "This data was produced by a party with knowledge of *one of* the following set of private keys" without identifying which specific private key -- if so, this would would help to push back in the direction of anonymity.
